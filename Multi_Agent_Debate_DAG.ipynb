{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3432ac93",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  Multi-Agent Debate DAG (LangGraph + LLMs)\n",
    "\n",
    "This notebook demonstrates a fully autonomous, multi-round debate simulation between two AI agents (Scientist and Philosopher) using Groq-hosted LLaMA-3 models and LangGraph.\n",
    "\n",
    "**Key Features:**\n",
    "- 8-round debate between two agents\n",
    "- Structured memory of arguments\n",
    "- Automated judging by a 70B model\n",
    "- CLI + Notebook compatibility\n",
    "\n",
    "**âš ï¸ Replace `GROQ_API_KEY` with your actual key**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install langgraph langchain langchain-core langchain-groq rich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4947b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"sk-your-key-here\"  # ðŸ” Replace with your own key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3030028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "class DebateAgent:\n",
    "    def __init__(self, name: str, model: str = \"llama3-8b-8192\"):\n",
    "        self.name = name\n",
    "        self.llm = ChatGroq(model_name=model, temperature=0.7)\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are {name}, an expert in debate. Debate the topic: {topic}.\"),\n",
    "            (\"human\", \"{history}\")\n",
    "        ])\n",
    "\n",
    "    def respond(self, topic: str, history: List[Tuple[str, str]]) -> str:\n",
    "        transcript = \"\\n\".join([f\"{speaker}: {arg}\" for speaker, arg in history])\n",
    "        chain = self.prompt.partial(name=self.name) | self.llm\n",
    "        return chain.invoke({\"topic\": topic, \"history\": transcript}).content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1002d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DebateJudge:\n",
    "    def __init__(self, model: str = \"llama3-70b-8192\"):\n",
    "        self.llm = ChatGroq(model_name=model, temperature=0.2)\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", '''You are an expert debate judge. Provide:\n",
    "1. A comprehensive summary\n",
    "2. The winner (\"Scientist\" or \"Philosopher\")\n",
    "3. Justification for your decision'''),\n",
    "            (\"human\", \"Debate Topic: {topic}\\nDebate Transcript:\\n{transcript}\")\n",
    "        ])\n",
    "\n",
    "    def judge(self, topic: str, history: List[Tuple[str, str]]) -> dict:\n",
    "        transcript = \"\\n\".join([f\"{s}: {a}\" for s, a in history])\n",
    "        result = (self.prompt | self.llm).invoke({\"topic\": topic, \"transcript\": transcript}).content\n",
    "        winner = \"Scientist\" if \"scientist\" in result.lower() else \"Philosopher\" if \"philosopher\" in result.lower() else \"Draw\"\n",
    "        return {\"summary\": result, \"winner\": winner}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class DebateMemory:\n",
    "    def __init__(self):\n",
    "        self.topic = \"\"\n",
    "        self.history = []\n",
    "\n",
    "    def set_topic(self, topic: str):\n",
    "        self.topic = topic\n",
    "\n",
    "    def add_argument(self, speaker: str, argument: str):\n",
    "        self.history.append((speaker, argument))\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def get(self, _):\n",
    "        return self.topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f19431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "\n",
    "class UserInputNode:\n",
    "    def __call__(self, state: Dict) -> Dict:\n",
    "        topic = input(\"Enter topic: \")\n",
    "        state[\"memory\"].set_topic(topic)\n",
    "        return {\"topic\": topic, \"memory\": state[\"memory\"], \"round\": 0, \"next_speaker\": \"Scientist\"}\n",
    "\n",
    "class AgentNode:\n",
    "    def __init__(self, name: str, agent):\n",
    "        self.name = name\n",
    "        self.agent = agent\n",
    "\n",
    "    def __call__(self, state: Dict) -> Dict:\n",
    "        topic = state[\"memory\"].get(\"topic\")\n",
    "        history = state[\"memory\"].get_history()\n",
    "        response = self.agent.respond(topic, history)\n",
    "        state[\"memory\"].add_argument(self.name, response)\n",
    "        return {\"memory\": state[\"memory\"], \"last_speaker\": self.name, \"last_argument\": response}\n",
    "\n",
    "class RoundControlNode:\n",
    "    def __init__(self, max_rounds: int):\n",
    "        self.max_rounds = max_rounds\n",
    "\n",
    "    def __call__(self, state: Dict) -> Dict:\n",
    "        current_round = state.get(\"round\", 0) + 1\n",
    "        done = current_round >= self.max_rounds\n",
    "        next_speaker = \"Philosopher\" if state.get(\"last_speaker\") == \"Scientist\" else \"Scientist\"\n",
    "        return {\"round\": current_round, \"done\": done, \"next_speaker\": next_speaker, \"memory\": state[\"memory\"]}\n",
    "\n",
    "class JudgeNode:\n",
    "    def __init__(self, judge):\n",
    "        self.judge = judge\n",
    "\n",
    "    def __call__(self, state: Dict) -> Dict:\n",
    "        topic = state[\"memory\"].get(\"topic\")\n",
    "        history = state[\"memory\"].get_history()\n",
    "        result = self.judge.judge(topic, history)\n",
    "        print(\"ðŸ“¢ Judge Verdict:\", result)\n",
    "        return {\"judgment\": result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_graph(agent_a, agent_b, judge):\n",
    "    g = StateGraph(state_schema=dict)\n",
    "    g.add_node(\"user_input\", UserInputNode())\n",
    "    g.add_node(\"agent_a\", AgentNode(\"Scientist\", agent_a))\n",
    "    g.add_node(\"agent_b\", AgentNode(\"Philosopher\", agent_b))\n",
    "    g.add_node(\"round_control\", RoundControlNode(8))\n",
    "    g.add_node(\"judge\", JudgeNode(judge))\n",
    "    \n",
    "    g.set_entry_point(\"user_input\")\n",
    "    g.add_edge(\"user_input\", \"round_control\")\n",
    "    g.add_conditional_edges(\"round_control\", lambda s: \"judge\" if s.get(\"done\") else (\"agent_a\" if s[\"next_speaker\"] == \"Scientist\" else \"agent_b\"), {\n",
    "        \"agent_a\": \"agent_a\",\n",
    "        \"agent_b\": \"agent_b\",\n",
    "        \"judge\": \"judge\"\n",
    "    })\n",
    "    g.add_edge(\"agent_a\", \"round_control\")\n",
    "    g.add_edge(\"agent_b\", \"round_control\")\n",
    "    g.set_finish_point(\"judge\")\n",
    "    return g.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = DebateMemory()\n",
    "agent_a = DebateAgent(\"Scientist\")\n",
    "agent_b = DebateAgent(\"Philosopher\")\n",
    "judge = DebateJudge()\n",
    "\n",
    "app = build_graph(agent_a, agent_b, judge)\n",
    "\n",
    "state = {\"memory\": memory, \"input\": {\"prompt\": \"Enter topic:\", \"input_func\": input}}\n",
    "for output in app.stream(state):\n",
    "    print(output)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
